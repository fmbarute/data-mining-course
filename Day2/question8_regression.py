# Auto Dataset Simple Linear Regression Analysis
# Exercise 8: mpg vs horsepower relationship

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from statsmodels.formula.api import ols
import scipy.stats as stats
from sklearn.datasets import fetch_openml
import warnings

warnings.filterwarnings('ignore')

print("üöó Auto Dataset Simple Linear Regression Analysis")
print("=" * 60)

# ============================================================================
# Load Auto Dataset
# ============================================================================
print("\nüìÅ Loading Auto dataset...")

try:
    # Try to load from OpenML (Auto dataset)
    auto = fetch_openml('autompg', version=1, as_frame=True, parser='auto')
    df = auto.frame

    # Clean and prepare data
    df = df.rename(columns={
        'mpg': 'mpg',
        'horsepower': 'horsepower',
        'weight': 'weight',
        'acceleration': 'acceleration',
        'model_year': 'year',
        'origin': 'origin'
    })

    # Handle missing values and convert horsepower to numeric
    df['horsepower'] = pd.to_numeric(df['horsepower'], errors='coerce')
    df = df.dropna(subset=['mpg', 'horsepower'])

except:
    # Create synthetic Auto dataset if loading fails
    print("Creating synthetic Auto dataset...")
    np.random.seed(42)
    n = 392

    # Generate realistic horsepower values
    horsepower = np.random.gamma(2, 50) + 50  # Range ~50-400
    horsepower = np.clip(horsepower, 50, 400)

    # Generate mpg with realistic relationship to horsepower
    # Relationship: mpg decreases as horsepower increases
    mpg = 40 - 0.15 * horsepower + np.random.normal(0, 3, n)
    mpg = np.clip(mpg, 5, 50)

    df = pd.DataFrame({
        'mpg': mpg,
        'horsepower': horsepower
    })

print(f"‚úÖ Dataset loaded successfully!")
print(f"üìä Shape: {df.shape}")
print(f"üìã Variables: {list(df.columns)}")
print("\nFirst 5 rows:")
print(df[['mpg', 'horsepower']].head())

# ============================================================================
# Part (a): Simple Linear Regression with statsmodels
# ============================================================================
print("\n" + "=" * 60)
print("(a) SIMPLE LINEAR REGRESSION: mpg ~ horsepower")
print("=" * 60)

# Prepare data for regression
X = df['horsepower']
y = df['mpg']

# Add constant term for intercept
X_with_const = sm.add_constant(X)

# Fit the model using OLS
model = sm.OLS(y, X_with_const)
results = model.fit()

# Print comprehensive results
print("\nüìä REGRESSION RESULTS:")
print(results.summary())

# Extract key statistics
r_squared = results.rsquared
adj_r_squared = results.rsquared_adj
f_statistic = results.fvalue
f_pvalue = results.f_pvalue
coef_horsepower = results.params['horsepower']
coef_intercept = results.params['const']
se_horsepower = results.bse['horsepower']
t_stat_horsepower = results.tvalues['horsepower']
p_value_horsepower = results.pvalues['horsepower']

print(f"\nüîç KEY STATISTICAL INSIGHTS:")
print(f"=" * 40)

# (i) Is there a relationship?
print(f"\n(i) RELATIONSHIP EXISTENCE:")
print(f"‚Ä¢ F-statistic: {f_statistic:.2f}")
print(f"‚Ä¢ F p-value: {f_pvalue:.2e}")
print(f"‚Ä¢ t-statistic (horsepower): {t_stat_horsepower:.2f}")
print(f"‚Ä¢ p-value (horsepower): {p_value_horsepower:.2e}")

if p_value_horsepower < 0.05:
    print(f"‚úÖ YES - Strong evidence of relationship (p < 0.05)")
else:
    print(f"‚ùå NO - No significant relationship (p ‚â• 0.05)")

# (ii) How strong is the relationship?
print(f"\n(ii) RELATIONSHIP STRENGTH:")
print(f"‚Ä¢ R-squared: {r_squared:.4f}")
print(f"‚Ä¢ Adjusted R-squared: {adj_r_squared:.4f}")
print(f"‚Ä¢ Correlation coefficient: {np.sqrt(r_squared):.4f}")

if r_squared > 0.7:
    strength = "Very Strong"
elif r_squared > 0.5:
    strength = "Strong"
elif r_squared > 0.3:
    strength = "Moderate"
else:
    strength = "Weak"

print(f"üìà Interpretation: {strength} relationship")
print(f"   ({r_squared * 100:.1f}% of mpg variance explained by horsepower)")

# (iii) Positive or negative relationship?
print(f"\n(iii) RELATIONSHIP DIRECTION:")
print(f"‚Ä¢ Slope coefficient: {coef_horsepower:.6f}")
print(f"‚Ä¢ Standard error: {se_horsepower:.6f}")

if coef_horsepower > 0:
    direction = "POSITIVE"
    interpretation = "mpg increases as horsepower increases"
else:
    direction = "NEGATIVE"
    interpretation = "mpg decreases as horsepower increases"

print(f"üìä Direction: {direction}")
print(f"   Interpretation: {interpretation}")
print(f"   For every 1 unit increase in horsepower, mpg changes by {coef_horsepower:.4f}")

# (iv) Prediction with confidence and prediction intervals
print(f"\n(iv) PREDICTION FOR HORSEPOWER = 98:")
print(f"=" * 40)

# Point prediction
horsepower_new = 98
X_new = sm.add_constant([horsepower_new])
prediction = results.predict(X_new)[0]

print(f"üéØ Point Prediction: {prediction:.2f} mpg")

# 95% Confidence Interval for the mean response
conf_int = results.get_prediction(X_new).conf_int(alpha=0.05)
conf_lower, conf_upper = conf_int[0]

print(f"üìä 95% Confidence Interval: [{conf_lower:.2f}, {conf_upper:.2f}] mpg")
print(f"   (Confidence interval for the MEAN mpg of all cars with 98 horsepower)")

# 95% Prediction Interval for individual response
pred_int = results.get_prediction(X_new).conf_int(alpha=0.05, obs=True)
pred_lower, pred_upper = pred_int[0]

print(f"üîÆ 95% Prediction Interval: [{pred_lower:.2f}, {pred_upper:.2f}] mpg")
print(f"   (Prediction interval for a SINGLE car with 98 horsepower)")

print(f"\nüí° Interval Interpretation:")
print(f"‚Ä¢ Confidence interval is NARROWER (uncertainty about the mean)")
print(f"‚Ä¢ Prediction interval is WIDER (includes individual car variability)")

# ============================================================================
# Part (b): Scatter plot with regression line
# ============================================================================
print(f"\n" + "=" * 60)
print("(b) SCATTER PLOT WITH REGRESSION LINE")
print("=" * 60)

# Create figure with subplots
fig, ax = plt.subplots(1, 1, figsize=(10, 8))

# Scatter plot
ax.scatter(df['horsepower'], df['mpg'], alpha=0.6, color='steelblue', s=50)

# Add regression line using axline (if available) or manual calculation
x_range = np.linspace(df['horsepower'].min(), df['horsepower'].max(), 100)
y_pred = coef_intercept + coef_horsepower * x_range

ax.plot(x_range, y_pred, color='red', linewidth=2, label=f'Regression Line')

# Add prediction point
pred_point_x = horsepower_new
pred_point_y = prediction
ax.scatter(pred_point_x, pred_point_y, color='orange', s=100,
           label=f'Prediction (HP=98): {prediction:.1f} mpg', zorder=5)

# Formatting
ax.set_xlabel('Horsepower', fontsize=12)
ax.set_ylabel('Miles per Gallon (mpg)', fontsize=12)
ax.set_title('Simple Linear Regression: MPG vs Horsepower', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

# Add regression equation to plot
equation = f'mpg = {coef_intercept:.2f} + {coef_horsepower:.4f} √ó horsepower'
ax.text(0.05, 0.95, equation, transform=ax.transAxes, fontsize=11,
        bbox=dict(boxstyle="round,pad=0.3", facecolor="wheat", alpha=0.8))

# Add R-squared to plot
r_sq_text = f'R¬≤ = {r_squared:.3f}'
ax.text(0.05, 0.88, r_sq_text, transform=ax.transAxes, fontsize=11,
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.8))

plt.tight_layout()
plt.show()

print(f"‚úÖ Scatter plot with regression line created!")

# ============================================================================
# Part (c): Diagnostic plots
# ============================================================================
print(f"\n" + "=" * 60)
print("(c) DIAGNOSTIC PLOTS")
print("=" * 60)

# Calculate residuals and fitted values
fitted_values = results.fittedvalues
residuals = results.resid
standardized_residuals = residuals / np.sqrt(results.mse_resid)

# Create diagnostic plots
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Residuals vs Fitted Values
axes[0, 0].scatter(fitted_values, residuals, alpha=0.6, color='steelblue')
axes[0, 0].axhline(y=0, color='red', linestyle='--', alpha=0.8)
axes[0, 0].set_xlabel('Fitted Values')
axes[0, 0].set_ylabel('Residuals')
axes[0, 0].set_title('Residuals vs Fitted Values')
axes[0, 0].grid(True, alpha=0.3)

# Add LOWESS smooth line to detect patterns
try:
    from statsmodels.nonparametric.smoothers_lowess import lowess

    smoothed = lowess(residuals, fitted_values, frac=0.3)
    axes[0, 0].plot(smoothed[:, 0], smoothed[:, 1], color='orange', linewidth=2, label='LOWESS')
    axes[0, 0].legend()
except:
    pass

# 2. Q-Q Plot (Normal probability plot)
stats.probplot(standardized_residuals, dist="norm", plot=axes[0, 1])
axes[0, 1].set_title('Q-Q Plot (Normal Probability Plot)')
axes[0, 1].grid(True, alpha=0.3)

# 3. Scale-Location Plot (Square root of standardized residuals vs fitted)
sqrt_abs_resid = np.sqrt(np.abs(standardized_residuals))
axes[1, 0].scatter(fitted_values, sqrt_abs_resid, alpha=0.6, color='steelblue')
axes[1, 0].set_xlabel('Fitted Values')
axes[1, 0].set_ylabel('‚àö|Standardized Residuals|')
axes[1, 0].set_title('Scale-Location Plot')
axes[1, 0].grid(True, alpha=0.3)

# 4. Residuals vs Leverage (Cook's Distance)
# Calculate leverage and Cook's distance
leverage = results.get_influence().hat_matrix_diag
cooks_d = results.get_influence().cooks_distance[0]

axes[1, 1].scatter(leverage, standardized_residuals, alpha=0.6, color='steelblue')
axes[1, 1].set_xlabel('Leverage')
axes[1, 1].set_ylabel('Standardized Residuals')
axes[1, 1].set_title('Residuals vs Leverage')
axes[1, 1].grid(True, alpha=0.3)

# Add Cook's distance contours
x_range = np.linspace(0, max(leverage), 100)
for cook_level in [0.5, 1.0]:
    y_upper = np.sqrt(cook_level * len(df) * (1 - x_range) / x_range)
    y_lower = -y_upper
    axes[1, 1].plot(x_range, y_upper, 'r--', alpha=0.5,
                    label=f"Cook's D = {cook_level}" if cook_level == 0.5 else "")
    axes[1, 1].plot(x_range, y_lower, 'r--', alpha=0.5)

axes[1, 1].legend()

plt.tight_layout()
plt.show()

# ============================================================================
# Diagnostic Analysis and Interpretation
# ============================================================================
print(f"\nüîç DIAGNOSTIC PLOT ANALYSIS:")
print(f"=" * 40)

# 1. Linearity Assessment
residual_pattern = np.corrcoef(fitted_values, residuals)[0, 1]
print(f"\n1Ô∏è‚É£ LINEARITY ASSUMPTION:")
print(f"‚Ä¢ Residuals vs Fitted correlation: {residual_pattern:.4f}")

if abs(residual_pattern) < 0.1:
    linearity_status = "‚úÖ SATISFIED"
    linearity_comment = "Random scatter around zero - linear relationship appropriate"
else:
    linearity_status = "‚ö†Ô∏è POTENTIAL ISSUE"
    linearity_comment = "Pattern in residuals suggests non-linear relationship"

print(f"‚Ä¢ Status: {linearity_status}")
print(f"‚Ä¢ Comment: {linearity_comment}")

# 2. Homoscedasticity (Equal Variance)
fitted_low = fitted_values <= np.median(fitted_values)
var_low = np.var(residuals[fitted_low])
var_high = np.var(residuals[~fitted_low])
variance_ratio = var_high / var_low

print(f"\n2Ô∏è‚É£ HOMOSCEDASTICITY (Equal Variance):")
print(f"‚Ä¢ Variance ratio (high/low fitted): {variance_ratio:.2f}")

if 0.5 <= variance_ratio <= 2.0:
    homoscedasticity_status = "‚úÖ SATISFIED"
    homoscedasticity_comment = "Variance appears roughly constant"
else:
    homoscedasticity_status = "‚ö†Ô∏è POTENTIAL ISSUE"
    homoscedasticity_comment = "Evidence of heteroscedasticity (non-constant variance)"

print(f"‚Ä¢ Status: {homoscedasticity_status}")
print(f"‚Ä¢ Comment: {homoscedasticity_comment}")

# 3. Normality of Residuals
shapiro_stat, shapiro_p = stats.shapiro(residuals)
print(f"\n3Ô∏è‚É£ NORMALITY OF RESIDUALS:")
print(f"‚Ä¢ Shapiro-Wilk test statistic: {shapiro_stat:.4f}")
print(f"‚Ä¢ Shapiro-Wilk p-value: {shapiro_p:.4f}")

if shapiro_p > 0.05:
    normality_status = "‚úÖ SATISFIED"
    normality_comment = "Residuals appear normally distributed"
else:
    normality_status = "‚ö†Ô∏è POTENTIAL ISSUE"
    normality_comment = "Residuals may not be normally distributed"

print(f"‚Ä¢ Status: {normality_status}")
print(f"‚Ä¢ Comment: {normality_comment}")

# 4. Influential Observations
high_leverage = leverage > 2 * len(results.params) / len(df)
high_cooks = cooks_d > 4 / len(df)
influential_points = np.sum(high_leverage | high_cooks)

print(f"\n4Ô∏è‚É£ INFLUENTIAL OBSERVATIONS:")
print(f"‚Ä¢ High leverage points: {np.sum(high_leverage)}")
print(f"‚Ä¢ High Cook's distance points: {np.sum(high_cooks)}")
print(f"‚Ä¢ Total influential points: {influential_points}")

if influential_points <= len(df) * 0.05:  # Less than 5%
    influence_status = "‚úÖ ACCEPTABLE"
    influence_comment = "Few influential observations"
else:
    influence_status = "‚ö†Ô∏è INVESTIGATE"
    influence_comment = "Multiple influential observations may affect results"

print(f"‚Ä¢ Status: {influence_status}")
print(f"‚Ä¢ Comment: {influence_comment}")

# Overall Model Assessment
print(f"\nüìã OVERALL MODEL ASSESSMENT:")
print(f"=" * 40)
print(f"‚Ä¢ Model fits the data reasonably well (R¬≤ = {r_squared:.3f})")
print(f"‚Ä¢ Significant negative relationship between horsepower and mpg")
print(f"‚Ä¢ {r_squared * 100:.1f}% of variance in mpg explained by horsepower")

# Potential improvements
print(f"\nüí° POTENTIAL IMPROVEMENTS:")
if linearity_status == "‚ö†Ô∏è POTENTIAL ISSUE":
    print(f"‚Ä¢ Consider polynomial terms or transformation")
if homoscedasticity_status == "‚ö†Ô∏è POTENTIAL ISSUE":
    print(f"‚Ä¢ Consider weighted least squares or transformation")
if normality_status == "‚ö†Ô∏è POTENTIAL ISSUE":
    print(f"‚Ä¢ Consider robust regression methods")
if influence_status == "‚ö†Ô∏è INVESTIGATE":
    print(f"‚Ä¢ Investigate and possibly remove influential observations")

print(f"\n‚úÖ Analysis Complete!")
print(f"üìä The simple linear regression provides a good baseline model")
print(f"üöó Higher horsepower cars tend to have lower fuel efficiency (mpg)")